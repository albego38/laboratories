{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6323fe6-7192-4d27-a41b-a59d374652b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eba08412-1264-4c1a-ad3f-02d7144e2be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"input_path\", \"abfss://datalake@dlsmde01user.dfs.core.windows.net/bronze/\", \"Input Path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb562796-9d35-4c6e-8dc9-0471808e3600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🧱 DLT Table: `transactions_bronze`\n",
    "Esta tabla crea una capa **bronze** en el flujo de procesamiento de datos usando **Delta Live Tables (DLT)** con PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf07920-1c74-4151-8cb5-a3aebbe15080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/2092/command-6917130923330172-923502315\", line 14, in transactions_bronze\n    .load(dbutils.widgets.get(\"input_path\"))  # Carga los datos desde la ruta especificada en el widget\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/streaming/readwriter.py\", line 306, in load\n    return self._df(self._jreader.load(path))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 255, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o546.load.\n: Failure to initialize configuration for storage account dlsmde01user.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:723)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2156)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:281)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:251)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:163)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:60)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:159)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:253)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:375)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:356)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:448)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:84)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getWorkingDirectory(CredentialScopeFileSystem.scala:260)\n\tat org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:684)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:262)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaRead(DeltaValidation.scala:132)\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:210)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy155.call(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:855)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1939)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:317)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:315)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1815)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1937)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:168)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:320)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1937)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1936)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:412)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:328)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:326)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:310)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:405)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:153)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:139)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult$lzycompute(Flow.scala:511)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:511)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:229)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:228)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:511)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:252)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:252)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:511)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:401)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:398)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:453)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:449)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:620)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:601)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:259)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:113)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:240)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:318)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:314)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:755)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 166 more\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/2092/command-6917130923330172-923502315\", line 14, in transactions_bronze\n    .load(dbutils.widgets.get(\"input_path\"))  # Carga los datos desde la ruta especificada en el widget\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/streaming/readwriter.py\", line 306, in load\n    return self._df(self._jreader.load(path))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 255, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o546.load.\n: Failure to initialize configuration for storage account dlsmde01user.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:723)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2156)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:281)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:251)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:163)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:60)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:159)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:253)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:375)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:356)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:448)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:84)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getWorkingDirectory(CredentialScopeFileSystem.scala:260)\n\tat org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:684)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:262)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaRead(DeltaValidation.scala:132)\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:210)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy155.call(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:855)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1939)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:317)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:315)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1815)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1937)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:168)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:320)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1937)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1936)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:412)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:328)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:326)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:310)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:405)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:153)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:139)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult$lzycompute(Flow.scala:511)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:511)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:229)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:228)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:511)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:252)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:252)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:511)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:401)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:398)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:453)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:449)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:620)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:601)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:259)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:113)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:240)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:318)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:314)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:755)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 166 more\n\n\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"transactions_bronze\",\n",
    "  comment=\"Datos crudos de transacciones desde ADLS Gen2 usando Auto Loader\",\n",
    ")\n",
    "def transactions_bronze():\n",
    "    # Lee datos en tiempo real desde un almacenamiento en formato CSV\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")  # Especifica el formato de archivo como CSV\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Inferir tipos de columna automáticamente\n",
    "        .load(dbutils.widgets.get(\"input_path\"))  # Carga los datos desde la ruta especificada en el widget\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46e1e05d-dc91-4168-8056-8b5c7f57a06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🧪DLT Table: `transactions_silver`\n",
    "\n",
    "Esta tabla representa la **capa silver** del pipeline de datos, en la cual se aplican **validaciones y transformaciones** a los datos crudos de transacciones previamente ingestados en la capa bronze.\n",
    "\n",
    "### 📌 Descripción\n",
    "`transactions_silver` limpia y transforma los datos de la tabla `transactions_bronze` para asegurar su calidad y prepararlos para análisis más avanzados o para construir la capa gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8061c9-4571-4e77-8cf6-1578c8d8dc22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>transactions_silver</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>transactionId</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>userId</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>timestamp</td>\n",
       "   <td>timestamp</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>amount</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>currency</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>transactionType</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>merchantName</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>merchantCategory</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>paymentMethod</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>transactionStatus</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>deviceType</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>deviceOs</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>deviceIpAddress</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>idCity</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>userLocationCity</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>userLocationCountry</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>userLocationLatitude</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>userLocationLongitude</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>_rescued_data</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=2648433616180891#joblist/pipelines/create?initialSource=%2FUsers%2Falbego38%40ucm.es%2FDLT%20Engine&redirectNotebookId=3153048861375829\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dlt.table(\n",
    "  name=\"transactions_silver\",\n",
    "  comment=\"Datos de transacciones validados y transformados\"\n",
    ")\n",
    "def transactions_silver():\n",
    "    df = dlt.read_stream(\"transactions_bronze\")\n",
    "    return (\n",
    "        df.filter(\"transactionId IS NOT NULL AND amount IS NOT NULL\")\n",
    "        .withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "        .withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "        .withColumn(\"transactionType\", upper(col(\"transactionType\")))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b9fee81-1d2f-4ab8-99a5-31a196f52db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🏆 DLT Table: `transactions_gold`\n",
    "\n",
    "Esta tabla corresponde a la **capa gold** dentro del pipeline de datos, donde se generan **agregaciones finales listas para el análisis de negocio o consumo por dashboards**.\n",
    "\n",
    "### 📊 Métricas generadas\n",
    "`transactions_gold` calcula el **importe total de transacciones** agrupado por categoría de comercio (`merchantCategory`), lo que permite identificar el volumen de transacciones por sector comercial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8911c12-32da-451e-b88c-ef84859bb154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>transactions_gold</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>merchantCategory</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>total_amount</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=2648433616180891#joblist/pipelines/create?initialSource=%2FUsers%2Falbego38%40ucm.es%2FDLT%20Engine&redirectNotebookId=3153048861375829\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"transactions_gold\",\n",
    "  comment=\"Total de importe de transacciones por categoría de comercio\")\n",
    "def transactions_gold():\n",
    "    df = dlt.read_stream(\"transactions_silver\")\n",
    "    \n",
    "    total_amount_by_market = (\n",
    "        df.groupBy(\"merchantCategory\")\n",
    "        .agg(sum(\"amount\").alias(\"total_amount\"))\n",
    "        .withColumn(\"total_amount\", round(\"total_amount\", 2))\n",
    "    )\n",
    "\n",
    "    return total_amount_by_market"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT Engine",
   "widgets": {
    "input_path": {
     "currentValue": "abfss://datalake@dlsmde01user.dfs.core.windows.net/bronze/",
     "nuid": "cac14164-a904-4824-80b6-931f56091104",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://datalake@dlsmde01user.dfs.core.windows.net/bronze/",
      "label": "Input Path",
      "name": "input_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://datalake@dlsmde01user.dfs.core.windows.net/bronze/",
      "label": "Input Path",
      "name": "input_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}